## ğŸ“š 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (Import Libraries)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ

## ğŸ“‚ 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ì´ˆ íƒìƒ‰ (Data Loading & Inspection)

# ë°ì´í„°ì…‹ ë¡œë“œ

# ëª¨ë“  ì»¬ëŸ¼ì´ ë³´ì´ë„ë¡ ì„¤ì •
pd.set_option('display.max_columns', None)

# ìƒìœ„ 5ê°œ í–‰ í™•ì¸

# ì»¬ëŸ¼ëª… í™•ì¸

# ë°ì´í„° í¬ê¸°(Shape) í™•ì¸
print(f"The Number of Rows are {df.shape[0]}, and columns are {df.shape[1]}.")
" 0ì´ í–‰ 1ì´ ì—´"

# ë°ì´í„° ì •ë³´(Info) ë° ê²°ì¸¡ì¹˜(Null) í™•ì¸

## ğŸ§¹ 3. ë°ì´í„° ì „ì²˜ë¦¬ (Data Cleaning)

### 3-1. íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í˜•ë³€í™˜ (Cleaning & Type Conversion)

# 1. ê°€ê²© ë°ì´í„° ì „ì²˜ë¦¬ (â‚¹, ì½¤ë§ˆ ì œê±° -> float ë³€í™˜)
df['discounted_price'] = df['discounted_price'].str.replace(",",'')
df['discounted_price'] = df['discounted_price'].astype('float64')

# 2. í• ì¸ìœ¨ ë°ì´í„° ì „ì²˜ë¦¬ (% ì œê±° -> float ë³€í™˜ ë° ë¹„ìœ¨ ê³„ì‚°)
df['discount_percentage'] = df['discount_percentage'].str.replace('%','').astype('float64')
df['discount_percentage'] = df['discount_percentage'] / 100

# ë³€í™˜ ê²°ê³¼ í™•ì¸

# Rating ì»¬ëŸ¼ì˜ ê°’ ë¶„í¬ í™•ì¸ (ì´ìƒí•œ ë¬¸ìì—´ ì°¾ê¸°)
df['rating'].value_counts()

# '|' ë¼ëŠ” ì´ìƒí•œ ë¬¸ìê°€ í¬í•¨ëœ í–‰(Row) ì¡°íšŒ
df.query('rating == "|"')

## ğŸ› ï¸ 4. ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ë¶„ì„ (Type Conversion & Missing Value Analysis)

### 4-1. ì´ìƒì¹˜ ì²˜ë¦¬ ë° ìµœì¢… í˜•ë³€í™˜

# 1. Rating ì»¬ëŸ¼ì˜ ì´ìƒì¹˜('|')ë¥¼ '3.9'ë¡œ ëŒ€ì²´ í›„ ì‹¤ìˆ˜í˜• ë³€í™˜
df['rating'] = df['rating'].str.replace('|', '3.9').astype('float64')

# 2. Rating Count ì»¬ëŸ¼ì˜ ì½¤ë§ˆ ì œê±° í›„ ì‹¤ìˆ˜í˜• ë³€í™˜

# 3. ë³€í™˜ëœ ë°ì´í„° ì •ë³´ ì¬í™•ì¸ (ëª¨ë“  ìˆ˜ì¹˜ ì»¬ëŸ¼ì´ float64ë¡œ ë³€í–ˆëŠ”ì§€ í™•ì¸)

# ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸

# 1. ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)

# 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%) í™•ì¸
# ì „ì²´ ë°ì´í„° ëŒ€ë¹„ ëª‡ í¼ì„¼íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨
print("\n--- ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (Percentage) ---")
print(round(df.isnull().sum() / len(df) * 100, 2).sort_values(ascending=False))

## ğŸ§© 5. ê²°ì¸¡ì¹˜ ì‹œê°í™” ë° ì²˜ë¦¬ (Handling Missing Values)

### 5-1. ê²°ì¸¡ì¹˜ ì‹œê°í™” (Visualizing Missing Data)

# 1. ì „ì²´ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸

# 2. ê²°ì¸¡ì¹˜ ë¶„í¬ íˆíŠ¸ë§µ (Heatmap)

# 3. ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë§‰ëŒ€ê·¸ë˜í”„ (Percentage Plot)
# ì–´ë–¤ ì»¬ëŸ¼ì´ ì‹¬ê°í•˜ê²Œ ë¹„ì–´ìˆëŠ”ì§€ í•œëˆˆì— ë¹„êµí•©ë‹ˆë‹¤.

# 1. ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ í™•ì¸ (ì–´ë–¤ ë°ì´í„°ì¸ì§€ ëˆˆìœ¼ë¡œ í™•ì¸)

# 2. ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (Median Imputation)
# í‰ê· ê°’ì´ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— ì¤‘ì•™ê°’ì´ ë” ì•ˆì „í•©ë‹ˆë‹¤.
df['rating_count'] = df['rating_count'].fillna(value=df['rating_count'].median())

# 3. ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ (ê²°ì¸¡ì¹˜ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸)

## ğŸ” 6. ì¤‘ë³µ ë°ì´í„° í™•ì¸ ë° ê¸°ì´ˆ ì‹œê°í™” (Duplication Check & Basic Plotting)

### 6-1. ì¤‘ë³µ ë°ì´í„° ê²€ì¦ (Checking for Duplicates)

# 1. ì „ì²´ í–‰ ê¸°ì¤€ ì¤‘ë³µ í™•ì¸
# (Trueê°€ ë‚˜ì˜¤ë©´ ì¤‘ë³µì´ ìˆë‹¤ëŠ” ëœ»)
print(f"Has duplicates? : {df.duplicated().any()}")

# 2. íŠ¹ì • ì»¬ëŸ¼ ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ í™•ì¸ 
any_duplicates = df.duplicated(subset=['product_id', 'product_name', 'category', 'discounted_price',
       'actual_price', 'discount_percentage', 'rating', 'rating_count',
       'about_product', 'user_id', 'user_name', 'review_id', 'review_title',
       'review_content', 'img_link', 'product_link']).any()

print(f"Detailed Duplicate Check: {any_duplicates}")

# ì‚°ì ë„ ê·¸ë¦¬ê¸°

plt.grid(True, linestyle='--', alpha=0.5)

# íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°

## ğŸ”¢ 7. ë°ì´í„° ì¸ì½”ë”© ë° ìƒê´€ê´€ê³„ ë¶„ì„ (Encoding & Correlation)

### 7-1. ë ˆì´ë¸” ì¸ì½”ë”© (Label Encoding)
`Scikit-learn`ì˜ `LabelEncoder`ë¥¼ ì‚¬ìš©í•˜ì—¬ `product_id`, `category` ê°™ì€ ë¬¸ìì—´ ë°ì´í„°ë¥¼ `0, 1, 2...`ì™€ ê°™ì€ ê³ ìœ í•œ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

# ì¸ì½”ë” ìƒì„± ë° ë°ì´í„° ë³€í™˜
le = LabelEncoder()

cols_to_encode = [
    'product_id', 'category', 'review_id', 'review_content', 
    'product_name', 'user_name', 'about_product', 'user_id', 
    'review_title', 'img_link', 'product_link'
]

for col in cols_to_encode:
    df[col] = le.fit_transform(df[col])

# ë³€í™˜ëœ ë°ì´í„° í™•ì¸

# 1. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (Pearson Correlation) - ì„ í˜• ê´€ê³„ í™•ì¸

# 2. ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ (Spearman Correlation) - ë¹„ì„ í˜•/ìˆœìœ„ ê´€ê³„ í™•ì¸
# (ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šê±°ë‚˜, ìˆœìœ„ê°€ ì¤‘ìš”í•  ë•Œ ìœ ìš©)


## ğŸ“‰ 8. ì‹¬í™” í†µê³„ ë¶„ì„ ë° í”¼ë²— í…Œì´ë¸” (Advanced Statistics & Aggregation)

ê·¸ë˜í”„ë¡œ í™•ì¸í•œ ê²½í–¥ì„±ì„ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¡œ ê²€ì¦í•˜ê³ , `groupby`ì™€ `pivot_table`ì„ í™œìš©í•´ ë°ì´í„°ë¥¼ ë‹¤ê°ë„ë¡œ ìš”ì•½

### 8-1. ìƒê´€ê³„ìˆ˜ ì •ë°€ ê³„ì‚° (Correlation Coefficient)

# 1. ì‹¤ì œ ê°€ê²©(Actual Price)ê³¼ í‰ì (Rating) ê°„ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
correlation_coefficient = np.corrcoef(df['actual_price'], df['rating'])[0, 1]
print(f"Correlation between Price and Rating: {correlation_coefficient:.4f}")

# 1. ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  í‰ì  (Mean Rating by Category)
mean_rating_by_category = df.groupby('category')['rating'].mean().sort_values(ascending=False)

# 2. ë¦¬ë·° ë‚´ìš©(ì¸ì½”ë”©ë¨)ë³„ í‰ì  ì¤‘ì•™ê°’ (Median Rating)
# (ì°¸ê³ : ì¸ì½”ë”©ëœ ì •ìˆ˜ê°’ ê¸°ì¤€ì´ë¯€ë¡œ ì˜ˆì‹œë¡œ í™•ì¸)
median_rating_by_review = df.groupby('review_content')['rating'].median()

# 3. ì œí’ˆëª…ë³„ ê°€ê²©ì˜ í‘œì¤€í¸ì°¨ (Standard Deviation)
# (ë™ì¼ ì œí’ˆëª…ì— ê°€ê²© ë³€ë™ì´ ìˆëŠ”ì§€ í™•ì¸)
std_price_by_product = df.groupby('product_name')['actual_price'].std()

# 1. ì¹´í…Œê³ ë¦¬(í–‰) x ì œí’ˆë§í¬(ì—´) ë³„ í‰ê·  í‰ì 
pivot_table_rating = df.pivot_table(values='rating', index='category', columns='product_link', aggfunc='mean')
print(pivot_table_rating.iloc[:5, :5]) # ë„ˆë¬´ í¬ë‹ˆ ì¼ë¶€ë¶„ë§Œ ì¶œë ¥

# 2. ë¦¬ë·°ë‚´ìš©(í–‰) x ì¹´í…Œê³ ë¦¬(ì—´) ë³„ í‰ê·  ë¦¬ë·° ìˆ˜(Rating Count)

## ğŸ§ª 9. ê°€ì„¤ ê²€ì • (Hypothesis Testing)

### 9-1. T-ê²€ì • (T-test)
ë‘ ì§‘ë‹¨(ì¹´í…Œê³ ë¦¬) ê°„ì˜ í‰ì  í‰ê· ì— ì§„ì§œ ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
import scipy.stats as stats

# 1. ë‘ ì§‘ë‹¨ì˜ í‰ì  ë°ì´í„° ì¶”ì¶œ
# (ë°ì´í„°ì— í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸ í•„ìš”)
group_electronics = df[df['category'] == 'electronics']['rating']
group_clothing = df[df['category'] == 'clothing']['rating']

# 2. T-test ìˆ˜í–‰
t_statistic, p_value = stats.ttest_ind(group_electronics, group_clothing)

# 3. ê²°ê³¼ ì¶œë ¥
print(f"T-statistic: {t_statistic}")
print(f"P-value: {p_value}")

if p_value < 0.05:
    print("ê²°ë¡ : ë‘ ì¹´í…Œê³ ë¦¬ì˜ í‰ì  ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•©ë‹ˆë‹¤.")
else:
    print("ê²°ë¡ : ë‘ ì¹´í…Œê³ ë¦¬ì˜ í‰ì  ì°¨ì´ëŠ” ìš°ì—°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 1. êµì°¨í‘œ(Contingency Table) ìƒì„±
contingency_table = pd.crosstab(df['actual_price'], df['rating'])
# print(contingency_table) # í…Œì´ë¸”ì´ ë„ˆë¬´ í¬ë©´ ìƒëµ

# 2. ì¹´ì´ì œê³± ê²€ì • ìˆ˜í–‰
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

# 3. ê²°ê³¼ ì¶œë ¥
# print(f"Expected:\n {expected}")
# LabelEncoder ê°ì²´(le_...)ê°€ ë©”ëª¨ë¦¬ì— ì‚´ì•„ìˆì–´ì•¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.

# ê° ì»¬ëŸ¼ë³„ë¡œ ì—­ë³€í™˜(Inverse Transform) ìˆ˜í–‰
df['product_id'] = le_product_id.inverse_transform(df['product_id'])

# ë³µì›ëœ ë°ì´í„° í™•ì¸