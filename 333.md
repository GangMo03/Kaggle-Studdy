## ğŸ“š 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (Import Libraries)

ë³¸ ë¶„ì„ì—ì„œëŠ” ë°ì´í„° ì²˜ë¦¬, ìˆ˜ì¹˜ ì—°ì‚°, ê·¸ë¦¬ê³  ì‹œê°í™”ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ íŒŒì´ì¬ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

1.  **Pandas**: ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ (Data manipulation and analysis)
2.  **Numpy**: ê³ ì„±ëŠ¥ ìˆ˜ì¹˜ ì—°ì‚° ë° ê³„ì‚° (Numerical operations and calculations)
3.  **Matplotlib**: ê¸°ë³¸ì ì¸ ë°ì´í„° ì‹œê°í™” ë° ê·¸ë˜í”„ ì‘ì„± (Data visualization and plotting)
4.  **Seaborn**: ê³ ê¸‰ ë°ì´í„° ì‹œê°í™” ë° í†µê³„ ê·¸ë˜í”½ (Enhanced data visualization and statistical graphics)
5.  **Scipy**: ê³¼í•™ì  ì»´í“¨íŒ… ë° ê³ ê¸‰ ìˆ˜í•™ ì—°ì‚° (Scientific computing and advanced mathematical operations)


# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp

# ì¥¬í”¼í„° ë…¸íŠ¸ë¶(Jupyter Notebook) ë‚´ì—ì„œ ê·¸ë˜í”„ë¥¼ ë°”ë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë§¤ì§ ì»¤ë§¨ë“œ
%matplotlib inline

## ğŸ“‚ 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ì´ˆ íƒìƒ‰ (Data Loading & Inspection)

ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³ , í–‰/ì—´ì˜ í¬ê¸°, ì»¬ëŸ¼ëª…, ë°ì´í„° íƒ€ì… ë“± ì „ë°˜ì ì¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

# ë°ì´í„°ì…‹ ë¡œë“œ
df = pd.read_csv("/kaggle/input/amazon-sales-dataset/amazon.csv")

# ëª¨ë“  ì»¬ëŸ¼ì´ ë³´ì´ë„ë¡ ì„¤ì •
pd.set_option('display.max_columns', None)

# ìƒìœ„ 5ê°œ í–‰ í™•ì¸
df.head(5)

# ì»¬ëŸ¼ëª… í™•ì¸
print(df.columns)

# ë°ì´í„° í¬ê¸°(Shape) í™•ì¸
print(f"The Number of Rows are {df.shape[0]}, and columns are {df.shape[1]}.")

# ë°ì´í„° ì •ë³´(Info) ë° ê²°ì¸¡ì¹˜(Null) í™•ì¸
df.info()
df.isnull().sum()

## ğŸ§¹ 3. ë°ì´í„° ì „ì²˜ë¦¬ (Data Cleaning)

ë¶„ì„ì„ ìœ„í•´ ë¬¸ìì—´(String)ë¡œ ë˜ì–´ ìˆëŠ” ìˆ«ì ë°ì´í„°ë¥¼ ì‹¤ì œ ìˆ˜ì¹˜í˜•(Float)ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë°ì´í„°ì— ì„ì—¬ ìˆëŠ” ì´ìƒì¹˜(Garbarge Data)ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.

### 3-1. íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° í˜•ë³€í™˜ (Cleaning & Type Conversion)
ê°€ê²© ë°ì´í„°(`â‚¹`, `,`)ì™€ í• ì¸ìœ¨(`%`)ì— í¬í•¨ëœ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ë¥¼ ì œê±°í•œ í›„, ê³„ì‚°ì´ ê°€ëŠ¥í•œ `float` í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.


# 1. ê°€ê²© ë°ì´í„° ì „ì²˜ë¦¬ (â‚¹, ì½¤ë§ˆ ì œê±° -> float ë³€í™˜)
df['discounted_price'] = df['discounted_price'].str.replace("â‚¹",'')
df['discounted_price'] = df['discounted_price'].str.replace(",",'')
df['discounted_price'] = df['discounted_price'].astype('float64')

df['actual_price'] = df['actual_price'].str.replace("â‚¹",'')
df['actual_price'] = df['actual_price'].str.replace(",",'')
df['actual_price'] = df['actual_price'].astype('float64')

# 2. í• ì¸ìœ¨ ë°ì´í„° ì „ì²˜ë¦¬ (% ì œê±° -> float ë³€í™˜ ë° ë¹„ìœ¨ ê³„ì‚°)
df['discount_percentage'] = df['discount_percentage'].str.replace('%','').astype('float64')
df['discount_percentage'] = df['discount_percentage'] / 100

# ë³€í™˜ ê²°ê³¼ í™•ì¸
print(df[['discounted_price', 'actual_price', 'discount_percentage']].head())

# Rating ì»¬ëŸ¼ì˜ ê°’ ë¶„í¬ í™•ì¸ (ì´ìƒí•œ ë¬¸ìì—´ ì°¾ê¸°)
df['rating'].value_counts()

# '|' ë¼ëŠ” ì´ìƒí•œ ë¬¸ìê°€ í¬í•¨ëœ í–‰(Row) ì¡°íšŒ
df.query('rating == "|"')

## ğŸ› ï¸ 4. ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ë¶„ì„ (Type Conversion & Missing Value Analysis)

ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜•(Float)ìœ¼ë¡œ ë³€í™˜ì„ ì™„ë£Œí•˜ê³ , ê¸°ì´ˆ í†µê³„ëŸ‰ê³¼ ê²°ì¸¡ì¹˜ í˜„í™©ì„ ë©´ë°€íˆ íŒŒì•…í•©ë‹ˆë‹¤.

### 4-1. ì´ìƒì¹˜ ì²˜ë¦¬ ë° ìµœì¢… í˜•ë³€í™˜
ì•ì„œ ë°œê²¬í•œ `rating` ì»¬ëŸ¼ì˜ ì´ìƒê°’(`|`)ì„ ì²˜ë¦¬í•˜ê³ , ì½¤ë§ˆë¥¼ ì œê±°í•˜ì—¬ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ í™•ì •í•©ë‹ˆë‹¤.


# 1. Rating ì»¬ëŸ¼ì˜ ì´ìƒì¹˜('|')ë¥¼ '3.9'ë¡œ ëŒ€ì²´ í›„ ì‹¤ìˆ˜í˜• ë³€í™˜
# (ì°¸ê³ : 3.9ëŠ” í•´ë‹¹ ë°ì´í„°ì˜ ì‹¤ì œ í‰ì ì´ê±°ë‚˜ ëŒ€ì²´ê°’ìœ¼ë¡œ íŒë‹¨ë¨)
df['rating'] = df['rating'].str.replace('|', '3.9').astype('float64')

# 2. Rating Count ì»¬ëŸ¼ì˜ ì½¤ë§ˆ ì œê±° í›„ ì‹¤ìˆ˜í˜• ë³€í™˜
df['rating_count'] = df['rating_count'].str.replace(',', '').astype('float64')

# 3. ë³€í™˜ëœ ë°ì´í„° ì •ë³´ ì¬í™•ì¸ (ëª¨ë“  ìˆ˜ì¹˜ ì»¬ëŸ¼ì´ float64ë¡œ ë³€í–ˆëŠ”ì§€ í™•ì¸)
df.info()

# ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸
df.describe()

# 1. ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
print("--- ê²°ì¸¡ì¹˜ ê°œìˆ˜ (Count) ---")
print(df.isnull().sum().sort_values(ascending=False))

# 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%) í™•ì¸
# ì „ì²´ ë°ì´í„° ëŒ€ë¹„ ëª‡ í¼ì„¼íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨
print("\n--- ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (Percentage) ---")
print(round(df.isnull().sum() / len(df) * 100, 2).sort_values(ascending=False))

## ğŸ§© 5. ê²°ì¸¡ì¹˜ ì‹œê°í™” ë° ì²˜ë¦¬ (Handling Missing Values)

ê²°ì¸¡ì¹˜ì˜ ë¶„í¬ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•œ ë’¤, ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ëŠ” ì ì ˆí•œ ê°’(ì¤‘ì•™ê°’ ë“±)ìœ¼ë¡œ ë¹ˆì¹¸ì„ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.

### 5-1. ê²°ì¸¡ì¹˜ ì‹œê°í™” (Visualizing Missing Data)
ìˆ«ìë§Œìœ¼ë¡œëŠ” íŒŒì•…í•˜ê¸° í˜ë“  **ê²°ì¸¡ì¹˜ì˜ íŒ¨í„´(ì–´ë””ì— ëª°ë ¤ìˆëŠ”ì§€)**ê³¼ **ë¹„ì¤‘**ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.


# 1. ì „ì²´ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
print(f"Total Missing Values: {df.isnull().sum().sum()}")

# 2. ê²°ì¸¡ì¹˜ ë¶„í¬ íˆíŠ¸ë§µ (Heatmap)
# ë…¸ë€ìƒ‰(ë˜ëŠ” ë°ì€ìƒ‰)ìœ¼ë¡œ í‘œì‹œëœ ë¶€ë¶„ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤.
# ë°ì´í„°ê°€ íŠ¹ì • êµ¬ê°„ì—ì„œ í†µì§¸ë¡œ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤.
plt.figure(figsize=(22, 10))
sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# 3. ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë§‰ëŒ€ê·¸ë˜í”„ (Percentage Plot)
# ì–´ë–¤ ì»¬ëŸ¼ì´ ì‹¬ê°í•˜ê²Œ ë¹„ì–´ìˆëŠ”ì§€ í•œëˆˆì— ë¹„êµí•©ë‹ˆë‹¤.
plt.figure(figsize=(22, 10))
missing_percentage = df.isnull().sum() / len(df) * 100
missing_percentage.plot(kind='bar', color='salmon')
plt.xlabel('Columns')
plt.ylabel('Percentage')
plt.title('Percentage of Missing Values in each Column')
plt.show()

# 1. ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ í™•ì¸ (ì–´ë–¤ ë°ì´í„°ì¸ì§€ ëˆˆìœ¼ë¡œ í™•ì¸)
df[df['rating_count'].isnull()].head(5)

# 2. ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (Median Imputation)
# í‰ê· (Mean) ëŒ€ì‹  ì¤‘ì•™ê°’(Median)ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :
# í‰ì  ê°œìˆ˜(Rating Count)ëŠ” ëª‡ëª‡ ì¸ê¸° ì œí’ˆì´ ì••ë„ì ìœ¼ë¡œ ë†’ì„ ìˆ˜ ìˆì–´(Outlier),
# í‰ê· ê°’ì´ ì™œê³¡ë  ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— ì¤‘ì•™ê°’ì´ ë” ì•ˆì „í•©ë‹ˆë‹¤.
df['rating_count'] = df['rating_count'].fillna(value=df['rating_count'].median())

# 3. ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ (ê²°ì¸¡ì¹˜ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸)
print("--- ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ í˜„í™© ---")
print(df.isnull().sum().sort_values(ascending=False))

## ğŸ” 6. ì¤‘ë³µ ë°ì´í„° í™•ì¸ ë° ê¸°ì´ˆ ì‹œê°í™” (Duplication Check & Basic Plotting)

ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ ìœ„í•´ ì¤‘ë³µëœ í–‰(Row)ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•˜ê³ , ê°€ê²©ê³¼ í‰ì  ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.

### 6-1. ì¤‘ë³µ ë°ì´í„° ê²€ì¦ (Checking for Duplicates)
ëª¨ë“  ì»¬ëŸ¼ì˜ ê°’ì´ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ ì¤‘ë³µ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì¤‘ë³µ ë°ì´í„°ëŠ” í†µê³„ ë¶„ì„ì— ì™œê³¡ì„ ì¤„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.


# 1. ì „ì²´ í–‰ ê¸°ì¤€ ì¤‘ë³µ í™•ì¸
# (Trueê°€ ë‚˜ì˜¤ë©´ ì¤‘ë³µì´ ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.)
print(f"Has duplicates? : {df.duplicated().any()}")

# 2. íŠ¹ì • ì»¬ëŸ¼ ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ í™•ì¸ (ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì»¬ëŸ¼ì„ ëª…ì‹œí–ˆìŠµë‹ˆë‹¤)
any_duplicates = df.duplicated(subset=['product_id', 'product_name', 'category', 'discounted_price',
       'actual_price', 'discount_percentage', 'rating', 'rating_count',
       'about_product', 'user_id', 'user_name', 'review_id', 'review_title',
       'review_content', 'img_link', 'product_link']).any()

print(f"Detailed Duplicate Check: {any_duplicates}")

import warnings
warnings.filterwarnings('ignore') # ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

# ì‚°ì ë„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 6))
plt.scatter(df['actual_price'], df['rating'], alpha=0.5, color='#b20710')
plt.xlabel('Actual Price (â‚¹)')
plt.ylabel('Rating (0-5)')
plt.title('Relationship between Actual Price and Rating')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

# íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 6))
plt.hist(df['actual_price'], bins=30, color='teal', edgecolor='black')
plt.xlabel('Actual Price (â‚¹)')
plt.ylabel('Frequency')
plt.title('Distribution of Actual Prices')
plt.show()

## ğŸ”¢ 7. ë°ì´í„° ì¸ì½”ë”© ë° ìƒê´€ê´€ê³„ ë¶„ì„ (Encoding & Correlation)

í…ìŠ¤íŠ¸ë¡œ ëœ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ì´ë‚˜ í†µê³„ ë¶„ì„ì´ ê°€ëŠ¥í•œ **ìˆ«ì í˜•íƒœ(Label Encoding)**ë¡œ ë³€í™˜í•˜ê³ , ë³€ìˆ˜ ê°„ì˜ **ìƒê´€ê´€ê³„(Correlation)**ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

### 7-1. ë ˆì´ë¸” ì¸ì½”ë”© (Label Encoding)
`Scikit-learn`ì˜ `LabelEncoder`ë¥¼ ì‚¬ìš©í•˜ì—¬ `product_id`, `category` ê°™ì€ ë¬¸ìì—´ ë°ì´í„°ë¥¼ `0, 1, 2...`ì™€ ê°™ì€ ê³ ìœ í•œ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> **ì£¼ì˜**: IDë‚˜ ë§í¬(Link), ë¦¬ë·° ë³¸ë¬¸(Content)ì²˜ëŸ¼ ê³ ìœ ê°’ì´ ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ìƒê´€ê´€ê³„ë¥¼ ë³´ëŠ” ê²ƒì€ í†µê³„ì ìœ¼ë¡œ í° ì˜ë¯¸ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•´ì„ì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤.


from sklearn.preprocessing import LabelEncoder

# ì¸ì½”ë” ìƒì„± ë° ë°ì´í„° ë³€í™˜
# (ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•˜ë©´ ì½”ë“œë¥¼ ë” ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ë³´ì—¬ë“œë¦¬ê¸° ìœ„í•´ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.)
le = LabelEncoder()

cols_to_encode = [
    'product_id', 'category', 'review_id', 'review_content', 
    'product_name', 'user_name', 'about_product', 'user_id', 
    'review_title', 'img_link', 'product_link'
]

for col in cols_to_encode:
    df[col] = le.fit_transform(df[col])

# ë³€í™˜ëœ ë°ì´í„° í™•ì¸
df.head()

# 1. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (Pearson Correlation) - ì„ í˜• ê´€ê³„ í™•ì¸
# (ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨)
plt.figure(figsize=(14, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt='.2f', linewidths=0.5)
plt.title("Correlation Matrix (Pearson - Linear Relationship)")
plt.show()

# 2. ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ (Spearman Correlation) - ë¹„ì„ í˜•/ìˆœìœ„ ê´€ê³„ í™•ì¸
# (ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šê±°ë‚˜, ìˆœìœ„ê°€ ì¤‘ìš”í•  ë•Œ ìœ ìš©í•¨)
plt.figure(figsize=(14, 10))
spearman_correlation_matrix = df.corr(method="spearman")
sns.heatmap(spearman_correlation_matrix, annot=True, cmap="coolwarm", fmt='.2f', linewidths=0.5)
plt.title("Correlation Matrix (Spearman - Rank Relationship)")
plt.show()

## ğŸ“‰ 8. ì‹¬í™” í†µê³„ ë¶„ì„ ë° í”¼ë²— í…Œì´ë¸” (Advanced Statistics & Aggregation)

ê·¸ë˜í”„ë¡œ í™•ì¸í•œ ê²½í–¥ì„±ì„ êµ¬ì²´ì ì¸ **ìˆ˜ì¹˜(Number)**ë¡œ ê²€ì¦í•˜ê³ , `groupby`ì™€ `pivot_table`ì„ í™œìš©í•´ ë°ì´í„°ë¥¼ ë‹¤ê°ë„ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

### 8-1. ìƒê´€ê³„ìˆ˜ ì •ë°€ ê³„ì‚° (Correlation Coefficient)
íˆíŠ¸ë§µì—ì„œ ìƒ‰ê¹”ë¡œë§Œ ë´¤ë˜ ìƒê´€ê´€ê³„ë¥¼ `numpy`ë¥¼ ì´ìš©í•´ ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ë½‘ì•„ë´…ë‹ˆë‹¤.


# 1. ì‹¤ì œ ê°€ê²©(Actual Price)ê³¼ í‰ì (Rating) ê°„ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
# (0.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ê³„ì—†ìŒ, 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ë¹„ë¡€)
correlation_coefficient = np.corrcoef(df['actual_price'], df['rating'])[0, 1]

print(f"Correlation between Price and Rating: {correlation_coefficient:.4f}")

# 1. ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  í‰ì  (Mean Rating by Category)
mean_rating_by_category = df.groupby('category')['rating'].mean().sort_values(ascending=False)
print("--- Mean Rating by Category ---")
print(mean_rating_by_category.head(5))

# 2. ë¦¬ë·° ë‚´ìš©(ì¸ì½”ë”©ë¨)ë³„ í‰ì  ì¤‘ì•™ê°’ (Median Rating)
# (ì°¸ê³ : ì¸ì½”ë”©ëœ ì •ìˆ˜ê°’ ê¸°ì¤€ì´ë¯€ë¡œ ì˜ˆì‹œë¡œ í™•ì¸)
median_rating_by_review = df.groupby('review_content')['rating'].median()
print("\n--- Median Rating by Review Content ---")
print(median_rating_by_review.head())

# 3. ì œí’ˆëª…ë³„ ê°€ê²©ì˜ í‘œì¤€í¸ì°¨ (Standard Deviation)
# (ë™ì¼ ì œí’ˆëª…ì— ê°€ê²© ë³€ë™ì´ ìˆëŠ”ì§€ í™•ì¸)
std_price_by_product = df.groupby('product_name')['actual_price'].std()
print("\n--- Price Standard Deviation by Product ---")
print(std_price_by_product.head())

# 1. ì¹´í…Œê³ ë¦¬(í–‰) x ì œí’ˆë§í¬(ì—´) ë³„ í‰ê·  í‰ì 
# (ë°ì´í„°ê°€ ë°©ëŒ€í•˜ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥í•˜ê±°ë‚˜ ì£¼ì˜ í•„ìš”)
pivot_table_rating = df.pivot_table(values='rating', index='category', columns='product_link', aggfunc='mean')
print("--- Pivot Table: Rating (Category x Link) ---")
print(pivot_table_rating.iloc[:5, :5]) # ë„ˆë¬´ í¬ë‹ˆ ì¼ë¶€ë¶„ë§Œ ì¶œë ¥

# 2. ë¦¬ë·°ë‚´ìš©(í–‰) x ì¹´í…Œê³ ë¦¬(ì—´) ë³„ í‰ê·  ë¦¬ë·° ìˆ˜(Rating Count)
pivot_table_count = df.pivot_table(values='rating_count', index='review_content', columns='category', aggfunc='mean')
print("\n--- Pivot Table: Rating Count (Review x Category) ---")
print(pivot_table_count.iloc[:5, :5])

## ğŸ§ª 9. ê°€ì„¤ ê²€ì • (Hypothesis Testing)

"ëˆˆìœ¼ë¡œ ë³´ê¸°ì— ì°¨ì´ê°€ ìˆëŠ”ë°, ì´ê²Œ ì§„ì§œ í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ìˆëŠ” ê±¸ê¹Œ?"ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

### 9-1. T-ê²€ì • (T-test)
ë‘ ì§‘ë‹¨(ì¹´í…Œê³ ë¦¬) ê°„ì˜ í‰ì  í‰ê· ì— ì§„ì§œ ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
* **ê°€ì„¤**: "ì „ìì œí’ˆ(Electronics)ê³¼ ì˜ë¥˜(Clothing)ì˜ í‰ì ì€ ë‹¤ë¥¼ ê²ƒì´ë‹¤."
* **íŒë‹¨ ê¸°ì¤€**: p-valueê°€ 0.05ë³´ë‹¤ ì‘ìœ¼ë©´ "í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆë‹¤"ê³  ë´…ë‹ˆë‹¤.

> **ì£¼ì˜**: ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì¹´í…Œê³ ë¦¬ëª…(`category`)ì´ 'electronics', 'clothing'ìœ¼ë¡œ ì •í™•íˆ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. (ëŒ€ì†Œë¬¸ì, ë„ì–´ì“°ê¸° ë“±)


import scipy.stats as stats

# 1. ë‘ ì§‘ë‹¨ì˜ í‰ì  ë°ì´í„° ì¶”ì¶œ
# (ë°ì´í„°ì— í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸ í•„ìš”)
group_electronics = df[df['category'] == 'electronics']['rating']
group_clothing = df[df['category'] == 'clothing']['rating']

# 2. T-test ìˆ˜í–‰
t_statistic, p_value = stats.ttest_ind(group_electronics, group_clothing)

# 3. ê²°ê³¼ ì¶œë ¥
print(f"T-statistic: {t_statistic}")
print(f"P-value: {p_value}")

if p_value < 0.05:
    print("ê²°ë¡ : ë‘ ì¹´í…Œê³ ë¦¬ì˜ í‰ì  ì°¨ì´ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•©ë‹ˆë‹¤.")
else:
    print("ê²°ë¡ : ë‘ ì¹´í…Œê³ ë¦¬ì˜ í‰ì  ì°¨ì´ëŠ” ìš°ì—°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 1. êµì°¨í‘œ(Contingency Table) ìƒì„±
contingency_table = pd.crosstab(df['actual_price'], df['rating'])
# print(contingency_table) # í…Œì´ë¸”ì´ ë„ˆë¬´ í¬ë©´ ìƒëµ

# 2. ì¹´ì´ì œê³± ê²€ì • ìˆ˜í–‰
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

# 3. ê²°ê³¼ ì¶œë ¥
print(f'Chi-square statistic: {chi2}')
print(f'p-value: {p}')
print(f'Degrees of freedom: {dof}')
# print(f"Expected:\n {expected}")

# LabelEncoder ê°ì²´(le_...)ê°€ ë©”ëª¨ë¦¬ì— ì‚´ì•„ìˆì–´ì•¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.

# ê° ì»¬ëŸ¼ë³„ë¡œ ì—­ë³€í™˜(Inverse Transform) ìˆ˜í–‰
df['product_id'] = le_product_id.inverse_transform(df['product_id'])
df['category'] = le_category.inverse_transform(df['category'])
df['review_id'] = le_review_id.inverse_transform(df['review_id'])
df['review_content'] = le_review_content.inverse_transform(df['review_content'])
df['product_name'] = le_product_name.inverse_transform(df['product_name'])
df['user_name'] = le_user_name.inverse_transform(df['user_name'])
df['about_product'] = le_about_product.inverse_transform(df['about_product'])
df['user_id'] = le_user_id.inverse_transform(df['user_id'])
df['review_title'] = le_review_title.inverse_transform(df['review_title'])
df['img_link'] = le_img_link.inverse_transform(df['img_link'])
df['product_link'] = le_product_link.inverse_transform(df['product_link'])

# ë³µì›ëœ ë°ì´í„° í™•ì¸
df.head()